{"cells": [{"cell_type": "markdown", "metadata": {"index": 0}, "source": ["# Standdown Exercise\n", "\n", "The cell below stores the text of a set of famous books in the variable nltk_books."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"index": 1}, "outputs": [], "source": ["# Run cell with no changes\n", "\n", "import nltk\n", "import pandas as pd\n", "\n", "# store raw text of books in a list\n", "nltk_books = [nltk.corpus.gutenberg.raw(title) \n", "                 for title in nltk.corpus.gutenberg.fileids()]\n", "\n", "# convert list to dataframe with titles as the index.\n", "nltk_books = pd.DataFrame(nltk_books, \n", "                          index=nltk.corpus.gutenberg.fileids(),\n", "                          columns=['raw_text'] )"]}, {"cell_type": "markdown", "metadata": {"index": 2}, "source": ["The next cell below splits the books into a train and test sets.  This is an arbitrary split, but is here to remind you that we fit a vectorizer only on the training set."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"index": 3}, "outputs": [], "source": ["# Run cell with no changes\n", "from sklearn.model_selection import train_test_split\n", "\n", "train, test = train_test_split(nltk_books, random_state=42)\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"index": 4}, "outputs": [{"data": {"text/plain": ["Index(['milton-paradise.txt', 'shakespeare-macbeth.txt',\n", "       'shakespeare-hamlet.txt', 'edgeworth-parents.txt', 'austen-sense.txt',\n", "       'chesterton-brown.txt', 'whitman-leaves.txt', 'blake-poems.txt',\n", "       'melville-moby_dick.txt', 'carroll-alice.txt',\n", "       'chesterton-thursday.txt', 'shakespeare-caesar.txt',\n", "       'burgess-busterbrown.txt'],\n", "      dtype='object')"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["# Here are the books whose full texts compose the training set\n", "train.index"]}, {"cell_type": "markdown", "metadata": {"index": 5}, "source": ["Your task is to fit a TfidfVectorizer to the training set with the following specifications: max_features is set to 50, stopwords are removed using the nltk english stopwords list.  The other parameters should be the defaults.  \n", "\n", "**After fitting the vectorizer, find the word with the highest tf-idf score in Moby Dick. Slack out both the word and tf-idf score, as well as your forked repo showing your work.**\n", "\n", "> Hint: Converting the vectorized text into a DataFrame with column names and indices will make your life easier.  Use the following hints to make that happen:  \n", ">> 1. The TF-IDF vectorizer returns a sparse matrix.  Chain the toarray() method off the vectorizer, then convert that array into a DataFrame.  \n", "\n", ">> 2. The fit Tf-Idf object has a method called `get_feature_names()`. Assign the result of that method as the `columns` argument of DataFrame.  \n", "\n", ">> 3. Pass train.index as the index argument of DataFrame.   \n", "    \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 7}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}